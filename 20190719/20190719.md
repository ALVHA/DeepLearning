
## 7월 19일 수업내용

* Gradient Descent Optimization Algorithm

* 신경망에서 Weight를 조절할 때 
* 매개변수 최적화기법 -> 일반적으로 Adam이 가장 좋은 결과를 보임
* 하지만 완벽하게 좋은 기법이 없음, 어느 상황이냐에 따라 다름


## SGD(Stochastic Gradient Descent)

* e단면이 타원이며 뭐 그렇..

## Momentum 

* 모멘텀은 운동량을 뜻함.
* 기울기가 음수이면 속도 증가, 양수이면 속도 감소
* a는 보통 0.9 로 설정

## AdaGrad
* 학습률 감소(Learning Rate Delay) 를 적용
* h의 원소가 각각의 매개변수 원소 변화량에 의해 결정


## RMSProp 
* AdaGrad 의 값이 무한히 성장하는 것을 방지

## Adam
* Momentum, AdaGrad 섞은 기법  (기울기와 속도를 모두 섞은 기법)
* 사람이 개입한 게 하이퍼파라미터, 모멘텀에서 사용하는 계수와 학습률에 대한 계수가 사용
* 학습률과 속도를 모두 조절
* 

## Optimizer     결론
* Gradient Descent
* 최적을 찾아가는 것은 정확하지만 느리다




```python

```


```python

```


```python

```


```python

```


```python

```
